{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c75a54",
   "metadata": {},
   "source": [
    "# Witful ML 08 - Regularized (LASSO & Ridge) Regression\n",
    "by Kaan Kabalak, Editor In Chief @ witfuldata.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf40491",
   "metadata": {},
   "source": [
    "# What is Regularized Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d093c3a",
   "metadata": {},
   "source": [
    "So far, we have gone through several main concepts of ML. From now on, we will go into some advanced topics. We shall start with regularized regression. So, what is it and why do we need it?\n",
    "\n",
    "\n",
    "To find an answer to these questions, we first have to understand feature coefficients. Don't let the fancy terminology scare you away. Feature coefficient is basically the measure of how strong a feature (X) variable affects the target(y) variable.\n",
    "\n",
    "\n",
    "When you are building a model it is almost always the best practice to avoid using too many features. Using too many features is bad for two main reasons:\n",
    "\n",
    "\n",
    "* Extra features mean extra computing. This results in needless waste of energy and time.\n",
    "\n",
    "* Using unneccessary features will cause the model to 'overlearn' (overfitting).\n",
    "\n",
    "\n",
    "Regularized Regression is here to solve these problems. It basically runs a linear regression model, but it also takes the coefficients into consideration. Every coefficient is used for the model in accordance with its effect on the outcome. There are different ways to do this and we will talk about them. This provides us with a better result than linear regression which does not do any calculations concerning the coefficients. Regularized regression also helps us select features for our model.\n",
    "\n",
    "\n",
    "In this chapter we will see how two main regularization methods are implemented with Scikit-learn. We will build a model which aims to predict price of a house based on various variables.\n",
    "\n",
    "\n",
    "## Note (with an amazing pun joke)\n",
    "\n",
    "I normally do not like using datasets which have become too mainstream (like the house dataset used in this tutorial). However, the dataset was very fit (pun intended) for the structure of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b225de",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8212f99",
   "metadata": {},
   "source": [
    "Variable,Explanation\n",
    "\n",
    "CRIM, The crime rate per capita\n",
    "\n",
    "ZN, The proportion of residential land zoned for lots over 25000 sq.ft\n",
    "\n",
    "INDUS, The proportion of non-retail business acres per town\n",
    "\n",
    "CHAS, The Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "NOX, The nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "RM, The average number of rooms per dwelling\n",
    "\n",
    "AGE, The proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "DIS, The weighted distances to five Boston employment centres\n",
    "\n",
    "RAD, The index of accessibility to radial highways\n",
    "\n",
    "TAX, The full-value property-tax rate per $ 10000\n",
    "\n",
    "PTRATIO, The pupil-teacher ratio by town\n",
    "\n",
    "LSTAT, The percentage lower status of the population\n",
    "\n",
    "PRICE, The median value of owner-occupied homes (in $1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12e0f3",
   "metadata": {},
   "source": [
    "## Prepare and Check the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9532cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da02346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO  LSTAT  PRICE  \n",
       "0     15.3   4.98   24.0  \n",
       "1     17.8   9.14   21.6  \n",
       "2     17.8   4.03   34.7  \n",
       "3     18.7   2.94   33.4  \n",
       "4     18.7   5.33   36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Form a data frame\n",
    "house_df = pd.read_csv('housing_data.csv')\n",
    "house_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12cf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  LSTAT    506 non-null    float64\n",
      " 12  PRICE    506 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 51.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Check the info\n",
    "house_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad47c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X-y\n",
    "X = house_df.drop('PRICE',axis=1).values\n",
    "\n",
    "y = house_df['PRICE'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42165384",
   "metadata": {},
   "source": [
    "# LASSO (Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88452a0",
   "metadata": {},
   "source": [
    "So, for the first one we have LASSO Regularization. What does it do?\n",
    "\n",
    "If we are to use the fancy data sciency term, it utilizes something called 'L1 Regularization'\n",
    "\n",
    "This sounds nice but what does it actually do?\n",
    "\n",
    "\n",
    "It plays around with the coefficients of the feature coefficients, pushing them towards zero. This process of readjusting coefficients is called 'penalization'. In a way, it punishes coefficients. The coefficients that actually become zero are left out of the model. What good does it do? Well, if we push every coefficient to zero, the ones which were already unimportant will eventually be zero. You can think of it like this:\n",
    "\n",
    "\n",
    "You are the coach of a football team. You are going to have a game with a rival team soon. The rival team does not play with traditional tactics. They run around a lot and tire their opponents. To compete with them your team needs endurance. Only the players who are able to withstand running for more than an hour should make it into the game. You need to predict their endurance in the game. In other words, your 'y' variable is player endurance. For 'X' variables you have many player attributes and skills. You don't have the time or the need to put these player attributes to test. You only need to see which attributes have an effect on 'y'. In Machine Learning terms, you have to select the correct X (player attribute) variables that have a high level of coefficient measurements when used to predict 'y' (endurance).\n",
    "\n",
    "\n",
    "What do you do for this?\n",
    "\n",
    "\n",
    "You make them run a lot in the training. Only the ones with attributes and skills related to endurance will be able to finish the training and play in the game. Making your players and tire them may seem like a punishment ('penalization'), but you do this for their good. \n",
    "\n",
    "\n",
    "\n",
    "This is the main logic behind LASSO models: Push every coefficient to zero (penalize them) and only the actually important ones for predicting y will be allowed into the model.\n",
    "\n",
    "\n",
    "Let's see how it works in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d469a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "lasso_model = Lasso(alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d6711",
   "metadata": {},
   "source": [
    "This alpha parameter is how hard the LASSO model will 'penalize' the coefficients. It is 1 by default, but 1 would be too 'harsh' for a dataset like this one. We will have a more detailed tutorial on how to tune parameters like alpha and why that actually matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d59c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "lasso_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8b266f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7466802230382312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model score\n",
    "lasso_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48d8cf",
   "metadata": {},
   "source": [
    "We can how many feature coefficients were pushed to 0 and left out of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c3191f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients\n",
    "sum(lasso_model.coef_ == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2e259",
   "metadata": {},
   "source": [
    "The model left one feature variable out of the model because it was not important enough. \n",
    "\n",
    "We can also visualize the coefficients after penalization like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c796ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's assign all the variables except the target variable (\"PRICE\") to the variable X\n",
    "X = house_df.drop(\"PRICE\",axis=1).values\n",
    "\n",
    "# Set the target variable\n",
    "y = house_df[\"PRICE\"].values\n",
    "\n",
    "# Select the feature variables\n",
    "feat_vars = house_df.drop(\"PRICE\",axis=1).columns\n",
    "\n",
    "# Fit a model for visualization\n",
    "viz_lasso_model= Lasso(alpha=0.1)\n",
    "\n",
    "# Get the coefficents for the variables\n",
    "lasso_model_coef = viz_lasso_model.fit(X,y).coef_\n",
    "\n",
    "#Visualize coefficents for each feature variable with a sns.barplot\n",
    "sns.barplot(x=feat_vars,y=lasso_model_coef)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bcffd0",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabfb1c",
   "metadata": {},
   "source": [
    "Ridge is another form of regularized regression. What makes it different than LASSO? \n",
    "\n",
    "\n",
    "Instead of pushing coefficients to zero, Ridge Regression focuses on 'weights'. You will hear this word 'weight' a lot when you study data science. It means taking something into consideration based on its importance. You can understand this by thinking about how you live your life. For example, if you have a an exam tomorrow, watching another episode of your favorite show is not that important. We can say that studying for the exam has a weight of 5, whereas watching the show has a weight of 1. \n",
    "\n",
    "\n",
    "Ridge Regression takes the feature coefficients, readjusts them by using weights based on their importance. It also penalizes the coefficients, but it does no do it harshly by pushing them into 0, like LASSO does. It just assigns new, and usually lower, values. No feature variable is left out of the model. \n",
    "\n",
    "\n",
    "Ridge Regression's way of doing things is called 'L2 Regularization'\n",
    "\n",
    "\n",
    "Let's see how we can use Ridge Regression in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8625b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "ridge_model = Ridge(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73607798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "ridge_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "350527fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7668521340499949"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model score\n",
    "ridge_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd60ddc",
   "metadata": {},
   "source": [
    "As you can see, Ridge Regression performed better than LASSO in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a97c7",
   "metadata": {},
   "source": [
    "## One more thing to know\n",
    "You should know that the real benefit of using regularized regression is much more clear when you use it on a very complex dataset with 30-40 features. In small scaled datasets like the one we have here, you shouldn't expect much difference when compared with a normal linear regression models. Sometimes, linear regression models may even perform a bit better than regularized regression models. Shortly put, it all depends on the dataset and your specific goals. Nonetheless, it will be good for you to know how to perform regularized regression and the main logic behind it. \n",
    "\n",
    "See you all in the next chapter of Witful ML!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
