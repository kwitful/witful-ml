{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815dfeb7",
   "metadata": {},
   "source": [
    "# Witful ML 09 - Hyperparameter Optimization\n",
    "by Kaan Kabalak, Editor In Chief @ witfuldata.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa370744",
   "metadata": {},
   "source": [
    "# What is hyperparameter optimization? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d1844",
   "metadata": {},
   "source": [
    "To understand this we have to answer the following question: What is a parameter?\n",
    "\n",
    "There are many fancy definitions out there. I want to explain what a parameter is through examples, within the context of machine learning.\n",
    "\n",
    "Do you remember the K-Nearest Neighbors classifier? We used a parameter there, 'k'. We assigned a value to this parameter so that the model could make the necessary computations accordingly. It used the k parameter we set to determine the number that defined 'nearest' neighbors. That 'k' was a parameter. <strong> Parameter can be defined as a part of a system (in this case, an algorithm) which affects the characteristic workflow of that system</strong>. When the k changes, the workflow of the KNN system-algorithm changes.\n",
    "\n",
    "A more recent example could be provided through our recent Regularized Regression tutorial. There we assigned a value to the 'alpha' parameter of the Lasso and Ridge models. The models used this 'alpha' parameter to determine how to penalize feature coefficients.\n",
    "\n",
    "Hyperparameter means the parameters we set before running our model. Like 'k' for Nearest Neighbors Classifier and 'alpha' for regularized regression. \n",
    "\n",
    "Hyperparameter optimization is about changing these parameters until we find the ones that work the best. Let's see how we can do that in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0d420",
   "metadata": {},
   "source": [
    "# What is Grid Search Cross Validation (And Why It's Not So Scary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba7aff",
   "metadata": {},
   "source": [
    "I know that this terms seems super complicated and a bit scary. Think about how simply we went through all those ML concepts up to this point. A thing is complicated not by itself, but by its definition. With the right definition, everything will become understandable for everyone. \n",
    "\n",
    "So let's talk about Grid Search Cross Validation (GridSearch CV) and why it's a good way to learn and use hyperparameter optimization. \n",
    "\n",
    "There are several ways to optimize parameters. In this tutorial we are going to focus on GridSearchCV because it is a good choice the understand the main logic of hyperparameter optimization. To understand how GridSearch CV works, we need to understand what K- Fold Cross Validation is and how it helps data scientists. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffeda5",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba933428",
   "metadata": {},
   "source": [
    "Think about the train_test split. We seperated a certain part of the dataset, gaining two different sets of data to train and test our model.\n",
    "\n",
    "The point of Hyperparameter Optimization is, as the name suggests, to find the best option possible. When we go over different parameters, trying out how models perform with them, we do this on a dataset with only one train_test split. This may not come in very handy for real life situations because other people can split the data in different ways. We need a method to make sure the parameters we are testing will perform well on datasets split in different ways. \n",
    "\n",
    "This is where Cross Validation comes to the rescue. It works like this:\n",
    "\n",
    "* Split the main dataset into any number of parts (here, we are going to make it 5)\n",
    "* Take one of these parts as the test set, and the other as the training set\n",
    "* Calculate the performance metric with the defined parameters (R2,Accuracy etc.)\n",
    "* Repeat the same process, each time using a different one of these 5 parts\n",
    "\n",
    "\n",
    "By the way, these split parts are called folds. Because there are various options about the number of these folds, we call it K-Fold, to highlight the fact that the number of folds can change.\n",
    "\n",
    "\n",
    "At the end of this process, we gain a lot of information. We will know how a certain parameter performed with a certain model on a dataset that was split in 5 different ways. We will have a profound opinion on how the parameter really affects the model performance in different scenarios.\n",
    "\n",
    "\n",
    "Let's see how to perform cross validation with KFold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d719f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efce5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame\n",
    "house_df = pd.read_csv('housing_data.csv')\n",
    "\n",
    "# Define X-y\n",
    "X = house_df.drop('PRICE', axis = 1).values\n",
    "y = house_df['PRICE'].values\n",
    "\n",
    "# Instantiate KFold, set shuffle to True to shuffle the dataset before splitting it into folds.\n",
    "kf = KFold(n_splits=5 , shuffle=True, random_state=4)\n",
    "\n",
    "# Instantiate the model\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "# Assign the results to a variable\n",
    "cv_res = cross_val_score (lasso_model, X, y, cv = kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c962976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70499706 0.71285768 0.72769772 0.76137373 0.62810964]\n"
     ]
    }
   ],
   "source": [
    "print (cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eade8a",
   "metadata": {},
   "source": [
    "These are the scores we have from the KFold cross validation.\n",
    "\n",
    "But how can we get an overall idea about it? By using NumPy! We can use the mean and std (standart deviation) methods of the NumPy to get a better idea about how the model would perform in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c14403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mean: 0.7070071657782433\n",
      "This is the standart deviation: 0.04392473238019104\n"
     ]
    }
   ],
   "source": [
    "print('This is the mean:', np.mean(cv_res))\n",
    "print('This is the standart deviation:', np.std(cv_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6e548",
   "metadata": {},
   "source": [
    "## Grid Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba9177",
   "metadata": {},
   "source": [
    "The main goal of cross validation is to see how a model with a certain parameter would perform on different dataset split scenarios. The main goal of Grid Search CV is similar, but this time it isn't just about splitting data. It is also about trying out models with different parameters (like 'alpha' for regularization or 'k' for neighbor models) on each of these data split scenarios and then evaluating their performance with different metrics (like R2 etc.)\n",
    "\n",
    "In other words Grid Search CV will provide us with a very big picture on our models and how they would perform in different scenarios. \n",
    "\n",
    "Let's see how we can perform Grid Search CV in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd054db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5b9621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=4, shuffle=True),\n",
       "             estimator=Lasso(), param_grid={'alpha': array([0.001])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate K-Fold\n",
    "kf = KFold (n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# Set the range of parameters\n",
    "param_grid = {'alpha':np.arange(0.001,10,10)}\n",
    "\n",
    "# Instantiate a model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Assign Grid Search to a variable\n",
    "lasso_grid = GridSearchCV (lasso, param_grid, cv = kf)\n",
    "\n",
    "# Fit GridSearch like you would do with a model\n",
    "lasso_grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2f3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "print ('Best parameter:',lasso_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcd0b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7206190370699099\n"
     ]
    }
   ],
   "source": [
    "# Get the best score\n",
    "print ('Best score:', lasso_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2432353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best estimator for prediction\n",
    "estimator_pred = lasso_grid.best_estimator_.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de2fe1",
   "metadata": {},
   "source": [
    "The best_estimator_ attribute refers to the best of all the models trained and tested with Grid Search CV. This can be used make predictions just as you would do with any machine learning model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
